{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a522d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "import ta as ta\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense,Dropout,LSTM,GRU,RNN,BatchNormalization,GaussianNoise\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "import keras\n",
    "import scipy\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "from tsmoothie.utils_func import sim_randomwalk\n",
    "from tsmoothie.smoother import *\n",
    "\n",
    "import time\n",
    "\n",
    "import ray\n",
    "\n",
    "\n",
    "def preparar_dados_financeiros(ticker,remove_out,start_date=\"\", end_date=\"\"):\n",
    "    base = yf.download(ticker)\n",
    "    \n",
    "    if base[\"Open\"].tail(1)[0] == 0:\n",
    "        base = base[:len(base)-1]\n",
    "    \n",
    "    if (start_date == \"\" or start_date == None) or (end_date == \"\" or end_date == None):\n",
    "        base = base\n",
    "    else:\n",
    "        base = base[start_date:end_date]\n",
    "        \n",
    "    base.dropna(inplace=True)\n",
    "    \n",
    "    if remove_out:\n",
    "        return remover_outliers(base)\n",
    "    else:\n",
    "        return base\n",
    "    \n",
    "def remover_outliers(base):\n",
    "    \n",
    "    z_scores = scipy.stats.zscore(base)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "    new_base = base[filtered_entries]\n",
    "    \n",
    "    return new_base\n",
    "\n",
    "\n",
    "def GetCci(base,normalizar=1):\n",
    "    \n",
    "    cci_config=[20,0.015]\n",
    "\n",
    "    resultados_cci = ta.trend.CCIIndicator(base[\"High\"],base[\"Low\"],base[\"Close\"],cci_config[0],cci_config[1],False)\n",
    "\n",
    "    cci_df = pd.DataFrame(resultados_cci.cci())\n",
    "    \n",
    "    cci_df.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "    if normalizar == 1:\n",
    "        \n",
    "        cci_normalizado = Normalizar(cci_df)\n",
    "        \n",
    "        return cci_normalizado\n",
    "            \n",
    "    return cci_df\n",
    "\n",
    "\n",
    "def GetTsi(base,gaussian_knots,gaussian_sigma,ewm_span=20):\n",
    "    \n",
    "    tsi_config=[25,13]\n",
    "\n",
    "    resultados_tsi = ta.momentum.TSIIndicator(base[\"Close\"],tsi_config[0],tsi_config[1],False)\n",
    "\n",
    "    tsi_df = pd.DataFrame(resultados_tsi.tsi())\n",
    "    \n",
    "    tsi_df.dropna(inplace=True)\n",
    "    \n",
    "    #Suavizando TSI com médias móveis exponenciais\n",
    "    tsi_df[\"ewm\"] = tsi_df['tsi'].ewm(span = ewm_span).mean()*1.2\n",
    "    #------------------------------------------\n",
    "    \n",
    "    #Suavizanto TSI com gaussian smoother\n",
    "    tsi_np = tsi_df[\"tsi\"].to_numpy()\n",
    "    tsi_np.reshape(1,len(tsi_np))\n",
    "\n",
    "    smoother = GaussianSmoother(n_knots=gaussian_knots, sigma=gaussian_sigma)\n",
    "    smoother.smooth(tsi_np)\n",
    "\n",
    "    tsi_df[\"gaussian\"] = smoother.smooth_data[0]\n",
    "    #------------------------------------------\n",
    "    \n",
    "    return tsi_df\n",
    "\n",
    "def GetRoc(base,gaussian_knots,gaussian_sigma,ewm_span=20):\n",
    "    \n",
    "    roc_config=[12]\n",
    "\n",
    "    resultados_roc = ta.momentum.ROCIndicator(base[\"Close\"],roc_config[0],False)\n",
    "    \n",
    "    roc_df = pd.DataFrame(resultados_roc.roc(),columns=[\"roc\"])\n",
    "    \n",
    "    roc_df.dropna(inplace=True)\n",
    "    \n",
    "    #Suavizando ROC com médias móveis exponenciais\n",
    "    roc_df[\"ewm\"] = roc_df['roc'].ewm(span = ewm_span).mean()*1.2\n",
    "    \n",
    "    #Suavizanto ROC com gaussian smoother\n",
    "    roc_np = roc_df[\"roc\"].to_numpy()\n",
    "    roc_np.reshape(1,len(roc_np))\n",
    "\n",
    "    smoother = GaussianSmoother(n_knots=gaussian_knots, sigma=gaussian_sigma)\n",
    "    smoother.smooth(roc_np)\n",
    "\n",
    "    roc_df[\"gaussian\"] = smoother.smooth_data[0]\n",
    "    #------------------------------------------\n",
    "    \n",
    "    return roc_df\n",
    "\n",
    "\n",
    "    \n",
    "def GetRsi(base,normalizar=1):\n",
    "    \n",
    "    rsi_config=[14,3,3]\n",
    "\n",
    "    resultados_rsi = ta.momentum.RSIIndicator(base[\"Close\"],rsi_config[0],False)\n",
    "\n",
    "    rsi_df = pd.DataFrame(resultados_rsi.rsi())\n",
    "    \n",
    "    rsi_df.dropna(inplace=True)\n",
    "    \n",
    "    if normalizar == 1:\n",
    "        \n",
    "        rsi_normalizado = Normalizar(rsi_df,0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return rsi_normalizado\n",
    "    \n",
    "    return rsi_df\n",
    "\n",
    "\n",
    "def Normalizar(Oscilador,coluna):\n",
    "    \n",
    "    normalizador = MinMaxScaler(feature_range=(0,1))\n",
    "    \n",
    "    if coluna == \"tsi\":\n",
    "        Oscilador_treinamento = Oscilador.iloc[:,0:1].values\n",
    "        \n",
    "    if coluna == \"ewm\":\n",
    "        Oscilador_treinamento = Oscilador.iloc[:,1:2].values\n",
    "        \n",
    "    if coluna == \"gaussian\":\n",
    "        Oscilador_treinamento = Oscilador.iloc[:,2:3].values\n",
    "        \n",
    "    Oscilador_normalizado = normalizador.fit_transform(Oscilador_treinamento)\n",
    "    \n",
    "    return Oscilador_normalizado\n",
    "\n",
    "def preparar_dados_para_treinamento(anteriores,base_treinamento_normalizada):\n",
    "\n",
    "    previsores = []\n",
    "    preco_real = []\n",
    "\n",
    "    for i in range(anteriores,len(base_treinamento_normalizada)):\n",
    "\n",
    "        previsores.append(base_treinamento_normalizada[i-anteriores:i,0])\n",
    "        preco_real.append(base_treinamento_normalizada[i,0])\n",
    "\n",
    "    previsores,preco_real = np.array(previsores),np.array(preco_real)\n",
    "    previsores = np.reshape(previsores,(previsores.shape[0],previsores.shape[1],1))\n",
    "    \n",
    "    return previsores,preco_real\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def criarRedeNeural(previsores,preco_real,filepath,epocas=300,validacao_cruzada=0,ativacao=\"linear\",otimizador=\"adam\",minimo_delta=1e-15,paciencia_es=10,batch=40):\n",
    "    \n",
    "    regressor = Sequential()\n",
    "    \n",
    "    #1º\n",
    "    regressor.add(LSTM(units=70,return_sequences=True,input_shape=(previsores.shape[1],1)))\n",
    "    regressor.add(Dropout(0.3))\n",
    "\n",
    "    #2º\n",
    "    regressor.add(LSTM(units=70,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "\n",
    "    #3º\n",
    "    regressor.add(LSTM(units=70,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #4º\n",
    "    regressor.add(LSTM(units=70,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #5º\n",
    "    regressor.add(LSTM(units=70,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    '''\n",
    "    #6º\n",
    "    regressor.add(LSTM(units=60,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #7º\n",
    "    regressor.add(LSTM(units=60,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #8º\n",
    "    regressor.add(LSTM(units=60,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #9º\n",
    "    regressor.add(LSTM(units=60,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #10º\n",
    "    regressor.add(LSTM(units=60,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #11º\n",
    "    regressor.add(LSTM(units=60,return_sequences=True))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    #12º\n",
    "    regressor.add(LSTM(units=60,return_sequences=True))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    #13º\n",
    "    regressor.add(LSTM(units=60,return_sequences=True))\n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    #14º\n",
    "    regressor.add(LSTM(units=80,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #15º\n",
    "    regressor.add(LSTM(units=100,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #16º\n",
    "    regressor.add(LSTM(units=100,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #17º\n",
    "    regressor.add(LSTM(units=100,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #18º\n",
    "    regressor.add(LSTM(units=100,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #19º\n",
    "    regressor.add(LSTM(units=100,return_sequences=True))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    '''\n",
    "    #20º\n",
    "    regressor.add(LSTM(units=70))\n",
    "    regressor.add(Dropout(0.3))\n",
    "    \n",
    "    #21º\n",
    "    regressor.add(Dense(units=1,activation=ativacao))\n",
    "    \n",
    "    \n",
    "    regressor.compile(optimizer=otimizador,loss='mean_squared_error',metrics=['mean_absolute_error'])\n",
    "    \n",
    "    es = EarlyStopping(monitor=\"loss\",min_delta=minimo_delta,patience = paciencia_es,verbose=1)    \n",
    "    rlr = ReduceLROnPlateau(monitor=\"loss\",factor=0.06,patience=5,verbose=1)\n",
    "    mcp = ModelCheckpoint(filepath=filepath,monitor=\"loss\",save_best_only=True,verbose=1)\n",
    "    \n",
    "    \n",
    "    if validacao_cruzada == 1:\n",
    "        \n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "        for train_index, test_index in kf.split(previsores):\n",
    "            X_train, X_test = previsores[train_index], previsores[test_index]\n",
    "            y_train, y_test = preco_real[train_index], preco_real[test_index]\n",
    "\n",
    "            regressor.fit(X_train, y_train, epochs=epocas, batch_size=batch,callbacks=[es,mcp])\n",
    "            score = regressor.evaluate(X_test, y_test, verbose=0)\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "            \n",
    "    else:\n",
    "\n",
    "        regressor.fit(previsores,preco_real,epochs=epocas,batch_size=batch,callbacks=[es,mcp])\n",
    "    \n",
    "    return regressor\n",
    "\n",
    "\n",
    "def criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas=300,validacao_cruzada=0,loss_='mean_squared_error',ativacao=\"linear\",otimizador=\"adam\",minimo_delta=1e-15,paciencia_es=10,batch=40):\n",
    "    \n",
    "    qtd_camadas-=2\n",
    "    \n",
    "    regressor = Sequential()\n",
    "    \n",
    "    i=0\n",
    "    while i < qtd_camadas:\n",
    "        \n",
    "        if i == 0:\n",
    "            regressor.add(LSTM(units=qtd_neuronios,return_sequences=True,input_shape=(previsores.shape[1],1)))\n",
    "            regressor.add(Dropout(dropout))\n",
    "            \n",
    "        else:\n",
    "            regressor.add(LSTM(units=qtd_neuronios,return_sequences=True))\n",
    "            regressor.add(Dropout(dropout))\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    \n",
    "    regressor.add(LSTM(units=qtd_neuronios))\n",
    "    regressor.add(Dropout(dropout))\n",
    "    \n",
    "    regressor.add(Dense(units=1,activation=ativacao))\n",
    "    \n",
    "    \n",
    "    regressor.compile(optimizer=otimizador,loss=loss_,metrics=['mean_absolute_error'])\n",
    "    \n",
    "    es = EarlyStopping(monitor=\"loss\",min_delta=minimo_delta,patience = paciencia_es,verbose=0)    \n",
    "    rlr = ReduceLROnPlateau(monitor=\"loss\",factor=0.06,patience=5,verbose=0)\n",
    "    mcp = ModelCheckpoint(filepath=filepath,monitor=\"loss\",save_best_only=True,verbose=0)\n",
    "    \n",
    "    if validacao_cruzada == 1:\n",
    "        \n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "        for train_index, test_index in kf.split(previsores):\n",
    "            X_train, X_test = previsores[train_index], previsores[test_index]\n",
    "            y_train, y_test = preco_real[train_index], preco_real[test_index]\n",
    "\n",
    "            regressor.fit(X_train, y_train, epochs=epocas, batch_size=batch,callbacks=[es,mcp],verbose=0)\n",
    "            score = regressor.evaluate(X_test, y_test, verbose=0)\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "            \n",
    "    else:\n",
    "\n",
    "        regressor.fit(previsores,preco_real,epochs=epocas,batch_size=batch,callbacks=[es,mcp],verbose=0)\n",
    "    \n",
    "    return regressor\n",
    "\n",
    "\n",
    "def Gaussian_1(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    print(\"Inicio Gaussian 1 \",filepath)\n",
    "    \n",
    "    anteriores = 40\n",
    "    \n",
    "    base = base.tail(1257)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 100\n",
    "    qtd_camadas = 8\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 32\n",
    "\n",
    "    modelo = criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 1 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "\n",
    "def Gaussian_1_ROC(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    print(\"Inicio Gaussian 1 \",filepath)\n",
    "    \n",
    "    anteriores = 40\n",
    "    \n",
    "    base = base.tail(1257)\n",
    "    \n",
    "    roc = GetRoc(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(roc,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 100\n",
    "    qtd_camadas = 8\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 32\n",
    "\n",
    "    modelo = criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 1 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "\n",
    "\n",
    "def Gaussian_1_GRU(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    print(\"Inicio Gaussian 1 GRU \",filepath)\n",
    "    \n",
    "    anteriores = 40\n",
    "    \n",
    "    base = base.tail(1257)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 100\n",
    "    qtd_camadas = 8\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 32\n",
    "\n",
    "    modelo = criarRedeNeural_custom_gru(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 1 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "\n",
    "\n",
    "def Gaussian_2(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    print(\"Inicio Gaussian 2 \",filepath)\n",
    "    \n",
    "    anteriores = 15\n",
    "    \n",
    "    base = base.tail(1257)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 60\n",
    "    qtd_camadas = 6\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 32\n",
    "\n",
    "    modelo = criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 2 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "\n",
    "def Gaussian_2_GRU(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    print(\"Inicio Gaussian 2 GRU \",filepath)\n",
    "    \n",
    "    anteriores = 15\n",
    "    \n",
    "    base = base.tail(1257)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 60\n",
    "    qtd_camadas = 6\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 32\n",
    "\n",
    "    modelo = criarRedeNeural_custom_gru(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 2 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "@ray.remote\n",
    "def Gaussian_3(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    anteriores = 40\n",
    "    \n",
    "    base = base.tail(1760)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 60\n",
    "    qtd_camadas = 10\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 40\n",
    "\n",
    "    modelo = criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 3 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "@ray.remote\n",
    "def Gaussian_4(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    anteriores = 15\n",
    "    \n",
    "    base = base.tail(1760)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 60\n",
    "    qtd_camadas = 7\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 40\n",
    "\n",
    "    modelo = criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 4 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "@ray.remote\n",
    "def Gaussian_5(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    anteriores = 40\n",
    "    \n",
    "    base = base.tail(1760)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 70\n",
    "    qtd_camadas = 10\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 40\n",
    "\n",
    "    modelo = criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 5 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "@ray.remote\n",
    "def Gaussian_6(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    anteriores = 90\n",
    "    \n",
    "    base = base.tail(1760)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 100\n",
    "    qtd_camadas = 10\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 32\n",
    "\n",
    "    modelo = criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 6 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "@ray.remote\n",
    "def Gaussian_10(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    anteriores = 15\n",
    "    \n",
    "    base = base.tail(1760)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 80\n",
    "    qtd_camadas = 6\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-15\n",
    "    paciencia_es = 10\n",
    "    batch = 20\n",
    "\n",
    "    modelo = criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 10 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "@ray.remote\n",
    "def Gaussian_11(base,filepath,k_nots=80,sigma=0.001):\n",
    "    \n",
    "    anteriores = 15\n",
    "    \n",
    "    base = base.tail(1760)\n",
    "    \n",
    "    tsi = GetTsi(base,k_nots,sigma)\n",
    "\n",
    "    normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "    previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "    #regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\n",
    "\n",
    "    qtd_neuronios = 60\n",
    "    qtd_camadas = 12\n",
    "    dropout = 0.3\n",
    "    epocas = 200\n",
    "    validacao_cruzada = 0\n",
    "    loss_ = 'mean_squared_error'\n",
    "    ativacao = 'linear'\n",
    "    otimizador = 'adam'\n",
    "    minimo_delta = 1e-10\n",
    "    paciencia_es = 10\n",
    "    batch = 20\n",
    "\n",
    "    modelo = criarRedeNeural_custom(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas,validacao_cruzada,loss_,ativacao,otimizador,minimo_delta,paciencia_es,batch)\n",
    "    \n",
    "    print(\"Gaussian 11 terminado \",filepath)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "\n",
    "def Criar_modelos_gaussian(tickers):\n",
    "    \n",
    "    \n",
    "    ray.init()\n",
    "\n",
    "    rfs = []\n",
    "    retornos = []\n",
    "    i = 0\n",
    "    \n",
    "    for ticker in tickers:\n",
    "\n",
    "        ticker_ = ticker[0].split(\".\")[0]\n",
    "        filepath = \"Modelos_\"+ticker_+\"/\"+ticker_\n",
    "        base = preparar_dados_financeiros(ticker[0],False)\n",
    "\n",
    "        g_k_nots = ticker[1]\n",
    "        g_sigma = ticker[2]\n",
    "\n",
    "        print(\"Começando treinamento do ativo \"+ticker_)\n",
    "    \n",
    "        rfs.append(Gaussian_1.remote(base,filepath+\"_Gaussian_1.h5\",g_k_nots,g_sigma))\n",
    "        rfs.append(Gaussian_2.remote(base,filepath+\"_Gaussian_2.h5\",g_k_nots,g_sigma))\n",
    "        rfs.append(Gaussian_3.remote(base,filepath+\"_Gaussian_3.h5\",g_k_nots,g_sigma))\n",
    "        rfs.append(Gaussian_4.remote(base,filepath+\"_Gaussian_4.h5\",g_k_nots,g_sigma))\n",
    "        rfs.append(Gaussian_5.remote(base,filepath+\"_Gaussian_5.h5\",g_k_nots,g_sigma))\n",
    "        rfs.append(Gaussian_6.remote(base,filepath+\"_Gaussian_6.h5\",g_k_nots,g_sigma))\n",
    "        rfs.append(Gaussian_10.remote(base,filepath+\"_Gaussian_10.h5\",g_k_nots,g_sigma))\n",
    "        rfs.append(Gaussian_11.remote(base,filepath+\"_Gaussian_11.h5\",g_k_nots,g_sigma))\n",
    "        retornos = ray.get(rfs)\n",
    "        \n",
    "        rfs= []\n",
    "    \n",
    "        i+=1    \n",
    "\n",
    "    ray.shutdown()\n",
    "    \n",
    "def Modelo_Brock_University(previsores,preco_real,filepath):\n",
    "    \n",
    "    epocas = 100\n",
    "    batch = 32\n",
    "    \n",
    "    regressor = Sequential()\n",
    "    \n",
    "    #LSTM 100 neuronios\n",
    "    regressor.add(LSTM(units=100,return_sequences=True,input_shape=(previsores.shape[1],1)))\n",
    "    \n",
    "    regressor.add(Dropout(0.2))\n",
    "   \n",
    "    #Gaussian Noise\n",
    "    regressor.add(GaussianNoise(0.05))\n",
    "    \n",
    "    #Batch normalization\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    #LSTM 200 neuronios\n",
    "    regressor.add(LSTM(units=200,return_sequences=True))\n",
    "    \n",
    "    regressor.add(Dropout(0.2))\n",
    "\n",
    "    #LSTM 50 neuronios\n",
    "    regressor.add(LSTM(units=50,return_sequences=True))\n",
    "    \n",
    "    regressor.add(Dropout(0.2))\n",
    "    \n",
    "    #regressor.add(Dense(units=1,activation=\"LeakyReLU\"))\n",
    "    regressor.add(Dense(units=1,activation=\"linear\"))\n",
    "    \n",
    "    regressor.compile(optimizer=\"Adam\",loss='mean_squared_error',metrics=['mean_absolute_error'])\n",
    "    \n",
    "    #es = EarlyStopping(monitor=\"loss\",min_delta=minimo_delta,patience = paciencia_es,verbose=1)    \n",
    "    #rlr = ReduceLROnPlateau(monitor=\"loss\",factor=0.06,patience=5,verbose=1)\n",
    "    mcp = ModelCheckpoint(filepath=filepath,monitor=\"loss\",save_best_only=True,verbose=1)\n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "    regressor.fit(previsores,preco_real,epochs=epocas,batch_size=batch,callbacks=[mcp])\n",
    "    \n",
    "    return regressor\n",
    "\n",
    "\n",
    "def criarRedeNeural_custom_gru(previsores,preco_real,filepath,qtd_neuronios,qtd_camadas,dropout,epocas=300,validacao_cruzada=0,loss_='mean_squared_error',ativacao=\"linear\",otimizador=\"adam\",minimo_delta=1e-15,paciencia_es=10,batch=40):\n",
    "    \n",
    "    qtd_camadas-=2\n",
    "    \n",
    "    regressor = Sequential()\n",
    "    \n",
    "    i=0\n",
    "    while i < qtd_camadas:\n",
    "        \n",
    "        if i == 0:\n",
    "            regressor.add(GRU(units=qtd_neuronios,return_sequences=True,input_shape=(previsores.shape[1],1)))\n",
    "            regressor.add(Dropout(dropout))\n",
    "            \n",
    "        else:\n",
    "            regressor.add(GRU(units=qtd_neuronios,return_sequences=True))\n",
    "            regressor.add(Dropout(dropout))\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    \n",
    "    regressor.add(GRU(units=qtd_neuronios))\n",
    "    regressor.add(Dropout(dropout))\n",
    "    \n",
    "    regressor.add(Dense(units=1,activation=ativacao))\n",
    "    \n",
    "    \n",
    "    regressor.compile(optimizer=otimizador,loss=loss_,metrics=['mean_absolute_error'])\n",
    "    \n",
    "    es = EarlyStopping(monitor=\"loss\",min_delta=minimo_delta,patience = paciencia_es,verbose=0)    \n",
    "    rlr = ReduceLROnPlateau(monitor=\"loss\",factor=0.06,patience=5,verbose=0)\n",
    "    mcp = ModelCheckpoint(filepath=filepath,monitor=\"loss\",save_best_only=True,verbose=0)\n",
    "    \n",
    "    if validacao_cruzada == 1:\n",
    "        \n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "        for train_index, test_index in kf.split(previsores):\n",
    "            X_train, X_test = previsores[train_index], previsores[test_index]\n",
    "            y_train, y_test = preco_real[train_index], preco_real[test_index]\n",
    "\n",
    "            regressor.fit(X_train, y_train, epochs=epocas, batch_size=batch,callbacks=[es,mcp],verbose=0)\n",
    "            score = regressor.evaluate(X_test, y_test, verbose=0)\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "            \n",
    "    else:\n",
    "\n",
    "        regressor.fit(previsores,preco_real,epochs=epocas,batch_size=batch,callbacks=[es,mcp],verbose=0)\n",
    "    \n",
    "    return regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6a3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"PETR3.SA\"\n",
    "\n",
    "anteriores = 12\n",
    "\n",
    "base = preparar_dados_financeiros(ticker,False\"2016\",\"2021\",)\n",
    "\n",
    "tsi = GetTsi(base,80,0.007)\n",
    "\n",
    "normalizado = Normalizar(tsi,\"gaussian\")\n",
    "\n",
    "previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "filepath = \"Modelos_PETR3\\Teste_Gaussian_10.h5\"\n",
    "qtd_neuronios = 80\n",
    "qtd_camadas = 6\n",
    "dropout = 0.3\n",
    "epocas = 200\n",
    "validacao_cruzada = 0\n",
    "loss_ = 'mean_squared_error'\n",
    "ativacao = 'linear'\n",
    "otimizador = 'adam'\n",
    "minimo_delta = 1e-15\n",
    "paciencia_es = 10\n",
    "batch = 20\n",
    "\n",
    "regressor = criarRedeNeural_custom(previsores,\n",
    "                                   preco_real,\n",
    "                                   filepath,\n",
    "                                   qtd_neuronios,\n",
    "                                   qtd_camadas,\n",
    "                                   dropout,epocas,\n",
    "                                   validacao_cruzada,\n",
    "                                   loss_,ativacao,\n",
    "                                   otimizador,\n",
    "                                   minimo_delta,\n",
    "                                   paciencia_es,\n",
    "                                   batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "\n",
    "\n",
    "ticker = \"PETR3.SA\"\n",
    "\n",
    "base = preparar_dados_financeiros(ticker,False)\n",
    "\n",
    "\n",
    "Gaussian_1(base,\"Modelos_TOTS3\\TOTS3_Gaussian_1.h5\",k_nots=80,sigma=0.002)\n",
    "Gaussian_2(base,\"Modelos_TOTS3\\TOTS3_Gaussian_2.h5\",k_nots=80,sigma=0.002)\n",
    "Gaussian_3(base,\"Modelos_TOTS3\\TOTS3_Gaussian_3.h5\",k_nots=80,sigma=0.002)\n",
    "Gaussian_4(base,\"Modelos_TOTS3\\TOTS3_Gaussian_4.h5\",k_nots=80,sigma=0.002)\n",
    "Gaussian_5(base,\"Modelos_TOTS3\\TOTS3_Gaussian_5.h5\",k_nots=80,sigma=0.002)\n",
    "Gaussian_6(base,\"Modelos_TOTS3\\TOTS3_Gaussian_6.h5\",k_nots=80,sigma=0.002)\n",
    "Gaussian_10(base,\"Modelos_TOTS3\\TOTS3_Gaussian_10.h5\",k_nots=80,sigma=0.002)\n",
    "Gaussian_11(base,\"Modelos_TOTS3\\TOTS3_Gaussian_11.h5\",k_nots=80,sigma=0.002)\n",
    "\n",
    "fim = time.time()\n",
    "\n",
    "\n",
    "print(\"Duração: \",fim - inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdbde36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 17:33:55,157\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Começando treinamento do ativo MDIA3\n"
     ]
    },
    {
     "ename": "RayTaskError(RuntimeError)",
     "evalue": "\u001b[36mray::Gaussian_3()\u001b[39m (pid=12912, ip=127.0.0.1)\n  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 274, in f\n    raise RuntimeError(\nRuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n\n\u001b[36mray::Gaussian_3()\u001b[39m (pid=12912, ip=127.0.0.1)\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::Gaussian_3()\u001b[39m (pid=12912, ip=127.0.0.1)\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 265, in fetch_and_register_remote_function\n    function = pickle.loads(serialized_function)\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py\", line 20, in <module>\n    from keras import distribute\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in <module>\n    from keras.distribute import sidecar_evaluator\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 17, in <module>\n    import tensorflow.compat.v2 as tf\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n    from tensorflow.python.tools import module_util as _module_util\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 36, in <module>\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 77, in <module>\n    raise ImportError(\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayTaskError(RuntimeError)\u001b[0m                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m tickers_7 \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mARZZ3.SA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m80\u001b[39m,\u001b[38;5;241m0.0035\u001b[39m],[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCYRE3.SA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m80\u001b[39m,\u001b[38;5;241m0.0035\u001b[39m],[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPETR3.SA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m80\u001b[39m,\u001b[38;5;241m0.007\u001b[39m]]\n\u001b[0;32m     20\u001b[0m inicio \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mCriar_modelos_gaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m fim \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mCriar_modelos_gaussian\u001b[1;34m(tickers)\u001b[0m\n\u001b[0;32m    621\u001b[0m rfs\u001b[38;5;241m.\u001b[39mappend(Gaussian_10\u001b[38;5;241m.\u001b[39mremote(base,filepath\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Gaussian_10.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m,g_k_nots,g_sigma))\n\u001b[0;32m    622\u001b[0m rfs\u001b[38;5;241m.\u001b[39mappend(Gaussian_11\u001b[38;5;241m.\u001b[39mremote(base,filepath\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_Gaussian_11.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m,g_k_nots,g_sigma))\n\u001b[1;32m--> 623\u001b[0m retornos \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m rfs\u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    627\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m    \n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py:2309\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2307\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[0;32m   2308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[1;32m-> 2309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[1;31mRayTaskError(RuntimeError)\u001b[0m: \u001b[36mray::Gaussian_3()\u001b[39m (pid=12912, ip=127.0.0.1)\n  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 274, in f\n    raise RuntimeError(\nRuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n\n\u001b[36mray::Gaussian_3()\u001b[39m (pid=12912, ip=127.0.0.1)\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::Gaussian_3()\u001b[39m (pid=12912, ip=127.0.0.1)\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 265, in fetch_and_register_remote_function\n    function = pickle.loads(serialized_function)\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py\", line 20, in <module>\n    from keras import distribute\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in <module>\n    from keras.distribute import sidecar_evaluator\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 17, in <module>\n    import tensorflow.compat.v2 as tf\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n    from tensorflow.python.tools import module_util as _module_util\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 36, in <module>\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 77, in <module>\n    raise ImportError(\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 17:34:06,765\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Gaussian_2()\u001b[39m (pid=8056, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 274, in f\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "\u001b[36mray::Gaussian_2()\u001b[39m (pid=8056, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::Gaussian_2()\u001b[39m (pid=8056, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 265, in fetch_and_register_remote_function\n",
      "    function = pickle.loads(serialized_function)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py\", line 20, in <module>\n",
      "    from keras import distribute\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in <module>\n",
      "    from keras.distribute import sidecar_evaluator\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 17, in <module>\n",
      "    import tensorflow.compat.v2 as tf\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 36, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 77, in <module>\n",
      "    raise ImportError(\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "See https://www.tensorflow.org/install/errors for some common causes and solutions.\n",
      "If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n",
      "2023-02-02 17:34:06,772\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Gaussian_11()\u001b[39m (pid=8056, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 274, in f\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "\u001b[36mray::Gaussian_11()\u001b[39m (pid=8056, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::Gaussian_11()\u001b[39m (pid=8056, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 265, in fetch_and_register_remote_function\n",
      "    function = pickle.loads(serialized_function)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py\", line 20, in <module>\n",
      "    from keras import distribute\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in <module>\n",
      "    from keras.distribute import sidecar_evaluator\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 17, in <module>\n",
      "    import tensorflow.compat.v2 as tf\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 36, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 77, in <module>\n",
      "    raise ImportError(\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "See https://www.tensorflow.org/install/errors for some common causes and solutions.\n",
      "If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n",
      "2023-02-02 17:34:06,776\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Gaussian_6()\u001b[39m (pid=10476, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 274, in f\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "\u001b[36mray::Gaussian_6()\u001b[39m (pid=10476, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::Gaussian_6()\u001b[39m (pid=10476, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 265, in fetch_and_register_remote_function\n",
      "    function = pickle.loads(serialized_function)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py\", line 20, in <module>\n",
      "    from keras import distribute\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in <module>\n",
      "    from keras.distribute import sidecar_evaluator\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 17, in <module>\n",
      "    import tensorflow.compat.v2 as tf\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 36, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 77, in <module>\n",
      "    raise ImportError(\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "See https://www.tensorflow.org/install/errors for some common causes and solutions.\n",
      "If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 17:34:06,782\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Gaussian_4()\u001b[39m (pid=7224, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 274, in f\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "\u001b[36mray::Gaussian_4()\u001b[39m (pid=7224, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::Gaussian_4()\u001b[39m (pid=7224, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 265, in fetch_and_register_remote_function\n",
      "    function = pickle.loads(serialized_function)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py\", line 20, in <module>\n",
      "    from keras import distribute\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in <module>\n",
      "    from keras.distribute import sidecar_evaluator\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 17, in <module>\n",
      "    import tensorflow.compat.v2 as tf\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 36, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 77, in <module>\n",
      "    raise ImportError(\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "See https://www.tensorflow.org/install/errors for some common causes and solutions.\n",
      "If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n",
      "2023-02-02 17:34:06,790\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Gaussian_10()\u001b[39m (pid=12912, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 274, in f\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "\u001b[36mray::Gaussian_10()\u001b[39m (pid=12912, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::Gaussian_10()\u001b[39m (pid=12912, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 265, in fetch_and_register_remote_function\n",
      "    function = pickle.loads(serialized_function)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py\", line 20, in <module>\n",
      "    from keras import distribute\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in <module>\n",
      "    from keras.distribute import sidecar_evaluator\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 17, in <module>\n",
      "    import tensorflow.compat.v2 as tf\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 36, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 77, in <module>\n",
      "    raise ImportError(\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "See https://www.tensorflow.org/install/errors for some common causes and solutions.\n",
      "If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n",
      "2023-02-02 17:34:06,799\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Gaussian_1()\u001b[39m (pid=10476, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 274, in f\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "\u001b[36mray::Gaussian_1()\u001b[39m (pid=10476, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::Gaussian_1()\u001b[39m (pid=10476, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 265, in fetch_and_register_remote_function\n",
      "    function = pickle.loads(serialized_function)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py\", line 20, in <module>\n",
      "    from keras import distribute\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in <module>\n",
      "    from keras.distribute import sidecar_evaluator\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 17, in <module>\n",
      "    import tensorflow.compat.v2 as tf\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 36, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 77, in <module>\n",
      "    raise ImportError(\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "See https://www.tensorflow.org/install/errors for some common causes and solutions.\n",
      "If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 17:34:06,807\tERROR worker.py:400 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::Gaussian_5()\u001b[39m (pid=7224, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 830, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 834, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 274, in f\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "\u001b[36mray::Gaussian_5()\u001b[39m (pid=7224, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::Gaussian_5()\u001b[39m (pid=7224, ip=127.0.0.1)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 265, in fetch_and_register_remote_function\n",
      "    function = pickle.loads(serialized_function)\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py\", line 20, in <module>\n",
      "    from keras import distribute\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in <module>\n",
      "    from keras.distribute import sidecar_evaluator\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 17, in <module>\n",
      "    import tensorflow.compat.v2 as tf\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 36, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 77, in <module>\n",
      "    raise ImportError(\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Facilimpa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n",
      "    from tensorflow.python._pywrap_tensorflow_internal import *\n",
      "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: O arquivo de paginação é muito pequeno para que esta operação seja concluída.\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "See https://www.tensorflow.org/install/errors for some common causes and solutions.\n",
      "If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tickers=[[\"MDIA3.SA\",80,0.003],[\"BRFS3.SA\",80,0.0037],[\"KLBN11.SA\",80,0.003],[\"SUZB3.SA\",80,0.004],\n",
    "         [\"B3SA3.SA\",80,0.0045],[\"ITUB4.SA\",80,0.0045],[\"BBDC3.SA\",80,0.0045],[\"ENGI11.SA\",80,0.0045],\n",
    "         [\"TAEE11.SA\",80,0.0045],[\"ELET6.SA\",80,0.003],[\"EGIE3.SA\",80,0.0037],[\"CSAN3.SA\",80,0.0037],\n",
    "         [\"RRRP3.SA\",80,0.0037],[\"TEND3.SA\",80,0.0037],[\"MOVI3.SA\",80,0.0045],[\"INTB3.SA\",80,0.0045],\n",
    "         [\"GMAT3.SA\",80,0.0045],[\"VIVA3.SA\",80,0.003],[\"LREN3.SA\",80,0.004],[\"SOMA3.SA\",80,0.004],\n",
    "         [\"RAIL3.SA\",80,0.003],[\"IGTI11.SA\",80,0.003],[\"ABEV3.SA\",80,0.004],[\"CRFB3.SA\",80,0.0027],\n",
    "         [\"ARZZ3.SA\",80,0.0035],[\"CYRE3.SA\",80,0.0035],[\"PETR3.SA\",80,0.007]]\n",
    "\n",
    "tickers_1 = [[\"MDIA3.SA\",80,0.003],[\"BRFS3.SA\",80,0.0037],[\"KLBN11.SA\",80,0.003],[\"SUZB3.SA\",80,0.004]]\n",
    "tickers_2 = [[\"B3SA3.SA\",80,0.0045],[\"ITUB4.SA\",80,0.0045],[\"BBDC3.SA\",80,0.0045],[\"ENGI11.SA\",80,0.0045]]\n",
    "tickers_3 = [[\"TAEE11.SA\",80,0.0045],[\"ELET6.SA\",80,0.003],[\"EGIE3.SA\",80,0.0037],[\"CSAN3.SA\",80,0.0037]]\n",
    "tickers_4 = [[\"RRRP3.SA\",80,0.0037],[\"TEND3.SA\",80,0.0037],[\"MOVI3.SA\",80,0.0045],[\"INTB3.SA\",80,0.0045]]\n",
    "tickers_5 = [[\"GMAT3.SA\",80,0.0045],[\"VIVA3.SA\",80,0.003],[\"LREN3.SA\",80,0.004],[\"SOMA3.SA\",80,0.004]]\n",
    "tickers_6 = [[\"RAIL3.SA\",80,0.003],[\"IGTI11.SA\",80,0.003],[\"ABEV3.SA\",80,0.004],[\"CRFB3.SA\",80,0.0027]]\n",
    "tickers_7 = [[\"ARZZ3.SA\",80,0.0035],[\"CYRE3.SA\",80,0.0035],[\"PETR3.SA\",80,0.007]]\n",
    "\n",
    "\n",
    "inicio = time.time()\n",
    "\n",
    "Criar_modelos_gaussian(tickers_1)\n",
    "\n",
    "fim = time.time()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(fim-inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118ab22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6debfece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Facilimpa\\AppData\\Local\\Temp\\ipykernel_528\\3262317519.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  base.dropna(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0662 - mean_absolute_error: 0.2011\n",
      "Epoch 1: loss improved from inf to 0.06623, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 17s 164ms/step - loss: 0.0662 - mean_absolute_error: 0.2011\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0400 - mean_absolute_error: 0.1582\n",
      "Epoch 2: loss improved from 0.06623 to 0.04001, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 6s 140ms/step - loss: 0.0400 - mean_absolute_error: 0.1582\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0368 - mean_absolute_error: 0.1521\n",
      "Epoch 3: loss improved from 0.04001 to 0.03676, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.0368 - mean_absolute_error: 0.1521\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.1497\n",
      "Epoch 4: loss improved from 0.03676 to 0.03534, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 8s 167ms/step - loss: 0.0353 - mean_absolute_error: 0.1497\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0343 - mean_absolute_error: 0.1483\n",
      "Epoch 5: loss improved from 0.03534 to 0.03435, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 8s 172ms/step - loss: 0.0343 - mean_absolute_error: 0.1483\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0337 - mean_absolute_error: 0.1469\n",
      "Epoch 6: loss improved from 0.03435 to 0.03372, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 9s 203ms/step - loss: 0.0337 - mean_absolute_error: 0.1469\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0341 - mean_absolute_error: 0.1479\n",
      "Epoch 7: loss did not improve from 0.03372\n",
      "45/45 [==============================] - 7s 164ms/step - loss: 0.0341 - mean_absolute_error: 0.1479\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0329 - mean_absolute_error: 0.1454\n",
      "Epoch 8: loss improved from 0.03372 to 0.03291, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 150ms/step - loss: 0.0329 - mean_absolute_error: 0.1454\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0328 - mean_absolute_error: 0.1447\n",
      "Epoch 9: loss improved from 0.03291 to 0.03281, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 8s 154ms/step - loss: 0.0328 - mean_absolute_error: 0.1447\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.1441\n",
      "Epoch 10: loss improved from 0.03281 to 0.03211, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 9s 197ms/step - loss: 0.0321 - mean_absolute_error: 0.1441\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.1438\n",
      "Epoch 11: loss improved from 0.03211 to 0.03208, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 159ms/step - loss: 0.0321 - mean_absolute_error: 0.1438\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.1426\n",
      "Epoch 12: loss improved from 0.03208 to 0.03160, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 9s 199ms/step - loss: 0.0316 - mean_absolute_error: 0.1426\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.1433\n",
      "Epoch 13: loss did not improve from 0.03160\n",
      "45/45 [==============================] - 7s 164ms/step - loss: 0.0319 - mean_absolute_error: 0.1433\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.1430\n",
      "Epoch 14: loss did not improve from 0.03160\n",
      "45/45 [==============================] - 7s 164ms/step - loss: 0.0318 - mean_absolute_error: 0.1430\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.1421\n",
      "Epoch 15: loss improved from 0.03160 to 0.03143, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 8s 177ms/step - loss: 0.0314 - mean_absolute_error: 0.1421\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.1415\n",
      "Epoch 16: loss improved from 0.03143 to 0.03120, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 155ms/step - loss: 0.0312 - mean_absolute_error: 0.1415\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.1427\n",
      "Epoch 17: loss did not improve from 0.03120\n",
      "45/45 [==============================] - 8s 176ms/step - loss: 0.0315 - mean_absolute_error: 0.1427\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.1420\n",
      "Epoch 18: loss did not improve from 0.03120\n",
      "45/45 [==============================] - 8s 173ms/step - loss: 0.0313 - mean_absolute_error: 0.1420\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.1414\n",
      "Epoch 19: loss improved from 0.03120 to 0.03110, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 8s 187ms/step - loss: 0.0311 - mean_absolute_error: 0.1414\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.1416\n",
      "Epoch 20: loss did not improve from 0.03110\n",
      "45/45 [==============================] - 8s 171ms/step - loss: 0.0312 - mean_absolute_error: 0.1416\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.1419\n",
      "Epoch 21: loss did not improve from 0.03110\n",
      "45/45 [==============================] - 7s 163ms/step - loss: 0.0312 - mean_absolute_error: 0.1419\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.1415\n",
      "Epoch 22: loss did not improve from 0.03110\n",
      "45/45 [==============================] - 7s 160ms/step - loss: 0.0312 - mean_absolute_error: 0.1415\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.1413\n",
      "Epoch 23: loss improved from 0.03110 to 0.03091, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 8s 157ms/step - loss: 0.0309 - mean_absolute_error: 0.1413\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.1418\n",
      "Epoch 24: loss did not improve from 0.03091\n",
      "45/45 [==============================] - 8s 169ms/step - loss: 0.0312 - mean_absolute_error: 0.1418\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.1423\n",
      "Epoch 25: loss did not improve from 0.03091\n",
      "45/45 [==============================] - 8s 175ms/step - loss: 0.0315 - mean_absolute_error: 0.1423\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.1415\n",
      "Epoch 26: loss did not improve from 0.03091\n",
      "45/45 [==============================] - 9s 203ms/step - loss: 0.0310 - mean_absolute_error: 0.1415\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.1416\n",
      "Epoch 27: loss did not improve from 0.03091\n",
      "45/45 [==============================] - 7s 147ms/step - loss: 0.0311 - mean_absolute_error: 0.1416\n",
      "Epoch 28/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.1420\n",
      "Epoch 28: loss did not improve from 0.03091\n",
      "45/45 [==============================] - 8s 174ms/step - loss: 0.0312 - mean_absolute_error: 0.1420\n",
      "Epoch 29/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1408\n",
      "Epoch 29: loss improved from 0.03091 to 0.03078, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 8s 172ms/step - loss: 0.0308 - mean_absolute_error: 0.1408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.1415\n",
      "Epoch 30: loss did not improve from 0.03078\n",
      "45/45 [==============================] - 7s 166ms/step - loss: 0.0310 - mean_absolute_error: 0.1415\n",
      "Epoch 31/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1408\n",
      "Epoch 31: loss did not improve from 0.03078\n",
      "45/45 [==============================] - 8s 182ms/step - loss: 0.0308 - mean_absolute_error: 0.1408\n",
      "Epoch 32/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.1413\n",
      "Epoch 32: loss did not improve from 0.03078\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0310 - mean_absolute_error: 0.1413\n",
      "Epoch 33/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.1414\n",
      "Epoch 33: loss did not improve from 0.03078\n",
      "45/45 [==============================] - 9s 196ms/step - loss: 0.0311 - mean_absolute_error: 0.1414\n",
      "Epoch 34/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1408\n",
      "Epoch 34: loss did not improve from 0.03078\n",
      "45/45 [==============================] - 7s 155ms/step - loss: 0.0308 - mean_absolute_error: 0.1408\n",
      "Epoch 35/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.1413\n",
      "Epoch 35: loss did not improve from 0.03078\n",
      "45/45 [==============================] - 7s 153ms/step - loss: 0.0311 - mean_absolute_error: 0.1413\n",
      "Epoch 36/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.1408\n",
      "Epoch 36: loss improved from 0.03078 to 0.03073, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 158ms/step - loss: 0.0307 - mean_absolute_error: 0.1408\n",
      "Epoch 37/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.1411\n",
      "Epoch 37: loss did not improve from 0.03073\n",
      "45/45 [==============================] - 6s 137ms/step - loss: 0.0309 - mean_absolute_error: 0.1411\n",
      "Epoch 38/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.1416\n",
      "Epoch 38: loss did not improve from 0.03073\n",
      "45/45 [==============================] - 6s 140ms/step - loss: 0.0310 - mean_absolute_error: 0.1416\n",
      "Epoch 39/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1410\n",
      "Epoch 39: loss did not improve from 0.03073\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0308 - mean_absolute_error: 0.1410\n",
      "Epoch 40/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.1414\n",
      "Epoch 40: loss did not improve from 0.03073\n",
      "45/45 [==============================] - 8s 169ms/step - loss: 0.0312 - mean_absolute_error: 0.1414\n",
      "Epoch 41/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1409\n",
      "Epoch 41: loss did not improve from 0.03073\n",
      "45/45 [==============================] - 8s 169ms/step - loss: 0.0308 - mean_absolute_error: 0.1409\n",
      "Epoch 42/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.1411\n",
      "Epoch 42: loss did not improve from 0.03073\n",
      "45/45 [==============================] - 7s 150ms/step - loss: 0.0309 - mean_absolute_error: 0.1411\n",
      "Epoch 43/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.1424\n",
      "Epoch 43: loss did not improve from 0.03073\n",
      "45/45 [==============================] - 7s 159ms/step - loss: 0.0314 - mean_absolute_error: 0.1424\n",
      "Epoch 44/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1410\n",
      "Epoch 44: loss did not improve from 0.03073\n",
      "45/45 [==============================] - 8s 174ms/step - loss: 0.0308 - mean_absolute_error: 0.1410\n",
      "Epoch 45/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1408\n",
      "Epoch 45: loss did not improve from 0.03073\n",
      "45/45 [==============================] - 7s 156ms/step - loss: 0.0308 - mean_absolute_error: 0.1408\n",
      "Epoch 46/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1406\n",
      "Epoch 46: loss improved from 0.03073 to 0.03065, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 149ms/step - loss: 0.0306 - mean_absolute_error: 0.1406\n",
      "Epoch 47/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1407\n",
      "Epoch 47: loss did not improve from 0.03065\n",
      "45/45 [==============================] - 7s 155ms/step - loss: 0.0308 - mean_absolute_error: 0.1407\n",
      "Epoch 48/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.1407\n",
      "Epoch 48: loss did not improve from 0.03065\n",
      "45/45 [==============================] - 7s 153ms/step - loss: 0.0307 - mean_absolute_error: 0.1407\n",
      "Epoch 49/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1404\n",
      "Epoch 49: loss improved from 0.03065 to 0.03058, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 149ms/step - loss: 0.0306 - mean_absolute_error: 0.1404\n",
      "Epoch 50/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.1406\n",
      "Epoch 50: loss did not improve from 0.03058\n",
      "45/45 [==============================] - 7s 148ms/step - loss: 0.0307 - mean_absolute_error: 0.1406\n",
      "Epoch 51/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1403\n",
      "Epoch 51: loss improved from 0.03058 to 0.03056, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 154ms/step - loss: 0.0306 - mean_absolute_error: 0.1403\n",
      "Epoch 52/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.1414\n",
      "Epoch 52: loss did not improve from 0.03056\n",
      "45/45 [==============================] - 7s 151ms/step - loss: 0.0309 - mean_absolute_error: 0.1414\n",
      "Epoch 53/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1409\n",
      "Epoch 53: loss did not improve from 0.03056\n",
      "45/45 [==============================] - 7s 150ms/step - loss: 0.0308 - mean_absolute_error: 0.1409\n",
      "Epoch 54/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1400\n",
      "Epoch 54: loss did not improve from 0.03056\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0306 - mean_absolute_error: 0.1400\n",
      "Epoch 55/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1403\n",
      "Epoch 55: loss did not improve from 0.03056\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0306 - mean_absolute_error: 0.1403\n",
      "Epoch 56/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1404\n",
      "Epoch 56: loss did not improve from 0.03056\n",
      "45/45 [==============================] - 7s 156ms/step - loss: 0.0306 - mean_absolute_error: 0.1404\n",
      "Epoch 57/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.1402\n",
      "Epoch 57: loss improved from 0.03056 to 0.03052, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0305 - mean_absolute_error: 0.1402\n",
      "Epoch 58/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.1403\n",
      "Epoch 58: loss did not improve from 0.03052\n",
      "45/45 [==============================] - 7s 150ms/step - loss: 0.0305 - mean_absolute_error: 0.1403\n",
      "Epoch 59/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1405\n",
      "Epoch 59: loss did not improve from 0.03052\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0306 - mean_absolute_error: 0.1405\n",
      "Epoch 60/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.1418\n",
      "Epoch 60: loss did not improve from 0.03052\n",
      "45/45 [==============================] - 7s 163ms/step - loss: 0.0310 - mean_absolute_error: 0.1418\n",
      "Epoch 61/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1404\n",
      "Epoch 61: loss did not improve from 0.03052\n",
      "45/45 [==============================] - 7s 147ms/step - loss: 0.0306 - mean_absolute_error: 0.1404\n",
      "Epoch 62/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.1405\n",
      "Epoch 62: loss did not improve from 0.03052\n",
      "45/45 [==============================] - 7s 162ms/step - loss: 0.0307 - mean_absolute_error: 0.1405\n",
      "Epoch 63/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1403\n",
      "Epoch 63: loss did not improve from 0.03052\n",
      "45/45 [==============================] - 7s 147ms/step - loss: 0.0306 - mean_absolute_error: 0.1403\n",
      "Epoch 64/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1409\n",
      "Epoch 64: loss did not improve from 0.03052\n",
      "45/45 [==============================] - 7s 160ms/step - loss: 0.0306 - mean_absolute_error: 0.1409\n",
      "Epoch 65/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.1403\n",
      "Epoch 65: loss improved from 0.03052 to 0.03051, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 154ms/step - loss: 0.0305 - mean_absolute_error: 0.1403\n",
      "Epoch 66/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1402\n",
      "Epoch 66: loss did not improve from 0.03051\n",
      "45/45 [==============================] - 7s 149ms/step - loss: 0.0306 - mean_absolute_error: 0.1402\n",
      "Epoch 67/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1404\n",
      "Epoch 67: loss did not improve from 0.03051\n",
      "45/45 [==============================] - 7s 158ms/step - loss: 0.0306 - mean_absolute_error: 0.1404\n",
      "Epoch 68/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1402\n",
      "Epoch 68: loss did not improve from 0.03051\n",
      "45/45 [==============================] - 7s 160ms/step - loss: 0.0306 - mean_absolute_error: 0.1402\n",
      "Epoch 69/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.1408\n",
      "Epoch 69: loss did not improve from 0.03051\n",
      "45/45 [==============================] - 7s 149ms/step - loss: 0.0307 - mean_absolute_error: 0.1408\n",
      "Epoch 70/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.1400\n",
      "Epoch 70: loss improved from 0.03051 to 0.03045, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 147ms/step - loss: 0.0305 - mean_absolute_error: 0.1400\n",
      "Epoch 71/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1406\n",
      "Epoch 71: loss did not improve from 0.03045\n",
      "45/45 [==============================] - 7s 146ms/step - loss: 0.0306 - mean_absolute_error: 0.1406\n",
      "Epoch 72/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1407\n",
      "Epoch 72: loss did not improve from 0.03045\n",
      "45/45 [==============================] - 7s 147ms/step - loss: 0.0308 - mean_absolute_error: 0.1407\n",
      "Epoch 73/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1403\n",
      "Epoch 73: loss did not improve from 0.03045\n",
      "45/45 [==============================] - 7s 150ms/step - loss: 0.0306 - mean_absolute_error: 0.1403\n",
      "Epoch 74/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.1405\n",
      "Epoch 74: loss did not improve from 0.03045\n",
      "45/45 [==============================] - 7s 155ms/step - loss: 0.0308 - mean_absolute_error: 0.1405\n",
      "Epoch 75/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.1399\n",
      "Epoch 75: loss did not improve from 0.03045\n",
      "45/45 [==============================] - 7s 151ms/step - loss: 0.0305 - mean_absolute_error: 0.1399\n",
      "Epoch 76/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1398\n",
      "Epoch 76: loss improved from 0.03045 to 0.03039, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.0304 - mean_absolute_error: 0.1398\n",
      "Epoch 77/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1402\n",
      "Epoch 77: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 149ms/step - loss: 0.0306 - mean_absolute_error: 0.1402\n",
      "Epoch 78/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.1401\n",
      "Epoch 78: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 150ms/step - loss: 0.0305 - mean_absolute_error: 0.1401\n",
      "Epoch 79/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1401\n",
      "Epoch 79: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0304 - mean_absolute_error: 0.1401\n",
      "Epoch 80/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1406\n",
      "Epoch 80: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 149ms/step - loss: 0.0306 - mean_absolute_error: 0.1406\n",
      "Epoch 81/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1401\n",
      "Epoch 81: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 155ms/step - loss: 0.0304 - mean_absolute_error: 0.1401\n",
      "Epoch 82/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1404\n",
      "Epoch 82: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0304 - mean_absolute_error: 0.1404\n",
      "Epoch 83/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.1405\n",
      "Epoch 83: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 153ms/step - loss: 0.0306 - mean_absolute_error: 0.1405\n",
      "Epoch 84/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.1402\n",
      "Epoch 84: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 153ms/step - loss: 0.0305 - mean_absolute_error: 0.1402\n",
      "Epoch 85/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.1418\n",
      "Epoch 85: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 154ms/step - loss: 0.0310 - mean_absolute_error: 0.1418\n",
      "Epoch 86/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1402\n",
      "Epoch 86: loss did not improve from 0.03039\n",
      "45/45 [==============================] - 7s 155ms/step - loss: 0.0304 - mean_absolute_error: 0.1402\n",
      "Epoch 87/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1400\n",
      "Epoch 87: loss improved from 0.03039 to 0.03037, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 155ms/step - loss: 0.0304 - mean_absolute_error: 0.1400\n",
      "Epoch 88/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1401\n",
      "Epoch 88: loss did not improve from 0.03037\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0304 - mean_absolute_error: 0.1401\n",
      "Epoch 89/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1400\n",
      "Epoch 89: loss did not improve from 0.03037\n",
      "45/45 [==============================] - 8s 169ms/step - loss: 0.0304 - mean_absolute_error: 0.1400\n",
      "Epoch 90/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1400\n",
      "Epoch 90: loss did not improve from 0.03037\n",
      "45/45 [==============================] - 7s 150ms/step - loss: 0.0304 - mean_absolute_error: 0.1400\n",
      "Epoch 91/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.1403\n",
      "Epoch 91: loss did not improve from 0.03037\n",
      "45/45 [==============================] - 7s 149ms/step - loss: 0.0305 - mean_absolute_error: 0.1403\n",
      "Epoch 92/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.1397\n",
      "Epoch 92: loss improved from 0.03037 to 0.03032, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 7s 153ms/step - loss: 0.0303 - mean_absolute_error: 0.1397\n",
      "Epoch 93/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1398\n",
      "Epoch 93: loss did not improve from 0.03032\n",
      "45/45 [==============================] - 7s 156ms/step - loss: 0.0304 - mean_absolute_error: 0.1398\n",
      "Epoch 94/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1399\n",
      "Epoch 94: loss did not improve from 0.03032\n",
      "45/45 [==============================] - 7s 155ms/step - loss: 0.0304 - mean_absolute_error: 0.1399\n",
      "Epoch 95/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.1399\n",
      "Epoch 95: loss did not improve from 0.03032\n",
      "45/45 [==============================] - 7s 153ms/step - loss: 0.0303 - mean_absolute_error: 0.1399\n",
      "Epoch 96/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1399\n",
      "Epoch 96: loss did not improve from 0.03032\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0304 - mean_absolute_error: 0.1399\n",
      "Epoch 97/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1401\n",
      "Epoch 97: loss did not improve from 0.03032\n",
      "45/45 [==============================] - 7s 148ms/step - loss: 0.0304 - mean_absolute_error: 0.1401\n",
      "Epoch 98/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1399\n",
      "Epoch 98: loss did not improve from 0.03032\n",
      "45/45 [==============================] - 7s 152ms/step - loss: 0.0304 - mean_absolute_error: 0.1399\n",
      "Epoch 99/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.1397\n",
      "Epoch 99: loss improved from 0.03032 to 0.03031, saving model to Modelos_PETR3\\Modelo_Brock_University2.h5\n",
      "45/45 [==============================] - 7s 164ms/step - loss: 0.0303 - mean_absolute_error: 0.1397\n",
      "Epoch 100/100\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.1399\n",
      "Epoch 100: loss did not improve from 0.03031\n",
      "45/45 [==============================] - 7s 161ms/step - loss: 0.0304 - mean_absolute_error: 0.1399\n"
     ]
    }
   ],
   "source": [
    "ticker = \"PETR3.SA\"\n",
    "\n",
    "anteriores = 45\n",
    "\n",
    "base = preparar_dados_financeiros(ticker,False,\"2016\",\"2021\")\n",
    "\n",
    "tsi = GetTsi(base,80,0.007)\n",
    "\n",
    "normalizado = Normalizar(tsi,\"tsi\")\n",
    "\n",
    "previsores,preco_real = preparar_dados_para_treinamento(anteriores,normalizado)\n",
    "\n",
    "filepath = \"Modelos_PETR3\\Modelo_Brock_University2.h5\"\n",
    "\n",
    "regressor = Modelo_Brock_University(previsores,preco_real,filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b0c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129780bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Inicio Gaussian 1  Modelos_PETR3\\PETR3_Gaussian_1_ROC.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(1245, 0)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m base \u001b[38;5;241m=\u001b[39m preparar_dados_financeiros(ticker,\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Gaussian_1(base,\"Modelos_PETR3\\PETR3_Gaussian_1.h5\",k_nots=80,sigma=0.007)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Gaussian_2(base,\"Modelos_PETR3\\PETR3_Gaussian_2.h5\",k_nots=80,sigma=0.007)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Gaussian_1_GRU(base,\"Modelos_PETR3\\PETR3_Gaussian_1_GRU.h5\",k_nots=80,sigma=0.007)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#Gaussian_2_GRU(base,\"Modelos_PETR3\\PETR3_Gaussian_2_GRU.h5\",k_nots=80,sigma=0.007)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mGaussian_1_ROC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModelos_PETR3\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPETR3_Gaussian_1_ROC.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mk_nots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0002\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mGaussian_1_ROC\u001b[1;34m(base, filepath, k_nots, sigma)\u001b[0m\n\u001b[0;32m    394\u001b[0m base \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m1257\u001b[39m)\n\u001b[0;32m    396\u001b[0m roc \u001b[38;5;241m=\u001b[39m GetRoc(base,k_nots,sigma)\n\u001b[1;32m--> 398\u001b[0m normalizado \u001b[38;5;241m=\u001b[39m \u001b[43mNormalizar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgaussian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m previsores,preco_real \u001b[38;5;241m=\u001b[39m preparar_dados_para_treinamento(anteriores,normalizado)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m#regressor = criarRedeNeural(previsores,preco_real,\"Modelos_SOMA3\\TSI_Gaussian_4.h5\",epocas=200)\u001b[39;00m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mNormalizar\u001b[1;34m(Oscilador, coluna)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coluna \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    160\u001b[0m     Oscilador_treinamento \u001b[38;5;241m=\u001b[39m Oscilador\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 162\u001b[0m Oscilador_normalizado \u001b[38;5;241m=\u001b[39m \u001b[43mnormalizador\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOscilador_treinamento\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Oscilador_normalizado\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:420\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:457\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMaxScaler does not support sparse input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using MaxAbsScaler instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     )\n\u001b[0;32m    456\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 457\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m data_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    465\u001b[0m data_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:918\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    916\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m<\u001b[39m ensure_min_features:\n\u001b[1;32m--> 918\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    919\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m feature(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    921\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_features, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m    922\u001b[0m         )\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[0;32m    925\u001b[0m     array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(1245, 0)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "ticker = \"PETR3.SA\"\n",
    "\n",
    "base = preparar_dados_financeiros(ticker,False)\n",
    "\n",
    "\n",
    "#Gaussian_1(base,\"Modelos_PETR3\\PETR3_Gaussian_1.h5\",k_nots=80,sigma=0.007)\n",
    "#Gaussian_2(base,\"Modelos_PETR3\\PETR3_Gaussian_2.h5\",k_nots=80,sigma=0.007)\n",
    "#Gaussian_1_GRU(base,\"Modelos_PETR3\\PETR3_Gaussian_1_GRU.h5\",k_nots=80,sigma=0.007)\n",
    "#Gaussian_2_GRU(base,\"Modelos_PETR3\\PETR3_Gaussian_2_GRU.h5\",k_nots=80,sigma=0.007)\n",
    "\n",
    "Gaussian_1_ROC(base,\"Modelos_PETR3\\PETR3_Gaussian_1_ROC.h5\",k_nots=60,sigma=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c88db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a89f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ffce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09c07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766dab6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
